{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb2894e6-26e2-4fec-9e4c-697fb4e03910",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install torch==2.0.0 torchvision==0.15.1 torchaudio==2.0.1 --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07ad691e-6604-42d0-a50e-679b98542ecc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade einops nltk pandas higher hydra-core numpy wandb datasets jsonlines matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28b5cee6-08d8-4932-889d-1a7f4826a788",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43686bc1-fbe2-4ac9-b50d-edc229dd9593",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install transformers==4.20.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda75d0c",
   "metadata": {},
   "source": [
    "# Editing a T5 QA model with GRACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc82f480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jui\\miniconda3\\envs\\grace_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import grace\n",
    "from grace.editors import GRACE_barebones as GRACE\n",
    "from grace.utils import tokenize_qa\n",
    "import torch\n",
    "import copy\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89338b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 605/605 [00:00<00:00, 603kB/s]\n",
      "Downloading: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 294M/294M [00:11<00:00, 28.0MB/s]\n",
      "Downloading: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2.02k/2.02k [00:00<00:00, 2.07MB/s]\n",
      "Downloading: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 773k/773k [00:00<00:00, 1.07MB/s]\n",
      "Downloading: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.74k/1.74k [00:00<00:00, 1.78MB/s]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/t5-small-ssm-nq\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/t5-small-ssm-nq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de96ce29",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_to_edit = \"encoder.block[4].layer[1].DenseReluDense.wo\" # Which layer to edit?\n",
    "init_epsilon = 3.0 # Initial epsilon for GRACE codebook entries\n",
    "learning_rate = 1.0 # Learning rate with which to learn new GRACE values\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "original_model = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b1e66bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edit_module T5DenseGatedActDense(\n",
      "  (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "  (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "  (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (act): NewGELUActivation()\n",
      ")\n",
      "layer_name wo\n",
      "original_layer Linear(in_features=1024, out_features=512, bias=False)\n",
      "Parameter containing:\n",
      "tensor([[ 8.4375e-01,  1.3184e-02, -7.4219e-01,  ..., -1.1641e+00,\n",
      "         -7.5684e-02,  8.5156e-01],\n",
      "        [ 8.8281e-01, -2.7930e-01, -3.0078e-01,  ...,  7.0312e-01,\n",
      "          8.8867e-02,  3.0859e-01],\n",
      "        [ 2.8992e-04,  3.3691e-02,  3.9844e-01,  ..., -2.9688e-01,\n",
      "          6.9922e-01,  2.5391e-01],\n",
      "        ...,\n",
      "        [ 5.3125e-01, -3.2812e-01,  2.5195e-01,  ...,  1.1250e+00,\n",
      "         -2.2168e-01,  6.7969e-01],\n",
      "        [ 4.7656e-01,  8.6914e-02, -7.2656e-01,  ...,  5.7422e-01,\n",
      "         -1.3906e+00,  6.0547e-01],\n",
      "        [-4.0234e-01, -7.5195e-02,  4.3359e-01,  ...,  5.0781e-02,\n",
      "         -3.1641e-01, -1.8652e-01]])\n"
     ]
    }
   ],
   "source": [
    "# --- wrap model with GRACE ---\n",
    "edited_model = GRACE(model, layer_to_edit, init_epsilon, learning_rate, device, generation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1784fa99-623d-4767-8dea-90ce86491deb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'encoder.block[4].layer[1].DenseReluDense.wo'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edited_model.layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e734240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Editing. Question: How tall is the empire state building?. Answer: 71 ft\n"
     ]
    }
   ],
   "source": [
    "# --- Desired edit ---\n",
    "edit_input = {\n",
    "    \"text\": [\"How tall is the empire state building?\"],\n",
    "    \"labels\": [\"1,454 feet\"],\n",
    "}\n",
    "\n",
    "edit_tokens = tokenize_qa(edit_input, tokenizer, device)\n",
    "\n",
    "# --- Check model's prediction for this edit before applying the edit ---\n",
    "preds = original_model.generate(edit_tokens[\"input_ids\"]).squeeze()\n",
    "original_answer = tokenizer.decode(preds, skip_special_tokens=True)\n",
    "print(f\"Before Editing. Question: {edit_input['text'][0]}. Answer: {original_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c0d3c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Apply the edit ---\n",
    "edited_model.edit(edit_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f86fb17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Editing. Question: How tall is the empire state building?. Answer: 1,454 feet\n"
     ]
    }
   ],
   "source": [
    "# --- Check model's prediction for this edit AFTER applying the edit ---\n",
    "preds = edited_model.generate(edit_tokens[\"input_ids\"]).squeeze()\n",
    "new_answer = tokenizer.decode(preds, skip_special_tokens=True)\n",
    "print(f\"After Editing. Question: {edit_input['text'][0]}. Answer: {new_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c808c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Editing. Question: how high is the empire state building?. Answer: 57 ft\n"
     ]
    }
   ],
   "source": [
    "# --- Trying slightly different input text ---\n",
    "test_input = {\n",
    "    \"text\": [\"how high is the empire state building?\"],\n",
    "    \"labels\": [\"1,454 feet\"]\n",
    "}\n",
    "\n",
    "test_tokens = tokenize_qa(test_input, tokenizer, device)\n",
    "\n",
    "preds = edited_model.generate(test_tokens[\"input_ids\"], max_length=20).squeeze()\n",
    "new_answer = tokenizer.decode(preds, skip_special_tokens=True)\n",
    "print(f\"After Editing. Question: {test_input['text'][0]}. Answer: {new_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58e9e2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Editing. Question: How tall is the eiffel tower?. Answer: 157 ft\n",
      "After Editing. Question: How tall is the eiffel tower?. Answer: 157 ft\n"
     ]
    }
   ],
   "source": [
    "# --- Check if the original and edited model have the same prediction on an unrelated input ---\n",
    "unrelated_input = {\n",
    "    \"text\": [\"How tall is the eiffel tower?\"],\n",
    "    \"labels\": [\"1,083 feet\"]\n",
    "}\n",
    "\n",
    "unrelated_tokens = tokenize_qa(unrelated_input, tokenizer, device)\n",
    "\n",
    "preds = original_model.generate(unrelated_tokens[\"input_ids\"]).squeeze()\n",
    "new_answer = tokenizer.decode(preds, skip_special_tokens=True)\n",
    "print(f\"Before Editing. Question: {unrelated_input['text'][0]}. Answer: {new_answer}\")\n",
    "\n",
    "preds = edited_model.generate(unrelated_tokens[\"input_ids\"]).squeeze()\n",
    "new_answer = tokenizer.decode(preds, skip_special_tokens=True)\n",
    "print(f\"After Editing. Question: {unrelated_input['text'][0]}. Answer: {new_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1504f05b-c265-41d7-8809-3377abc737fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec6fa25-b7ba-4360-9daa-bea27d8bef1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
